#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Section
Linear regression
\end_layout

\begin_layout Standard
Supervised learning is an approach where we are given the 
\begin_inset Quotes eld
\end_inset

correct
\begin_inset Quotes erd
\end_inset

 answer for each example in he data.
 Linear regression is a type of supervised learning.
\end_layout

\begin_layout Standard
There is some standard notation:
\end_layout

\begin_layout Itemize

\emph on
m
\emph default
 is the umber of training exampes
\end_layout

\begin_layout Itemize

\emph on
x
\emph default
 are the input variables/features
\end_layout

\begin_layout Itemize

\emph on
y
\emph default
 are the output variables/features
\end_layout

\begin_layout Itemize

\emph on
(x,y)
\emph default
 denotes a single training example
\end_layout

\begin_layout Itemize
\begin_inset Formula $(x^{(i)},y^{(i)})$
\end_inset

 - denotes 
\begin_inset Formula $i^{th}$
\end_inset

 training example
\end_layout

\begin_layout Itemize

\emph on
h 
\emph default
is the hypothesis function
\end_layout

\begin_layout Standard
\begin_inset Formula $h_{\theta}=\theta_{0}+\theta_{1}x$
\end_inset

 is a linear 
\emph on
hypothesis 
\emph default
function that predicts the outcome of 
\emph on
y 
\emph default
based on 
\emph on
x
\emph default
 and training set of 
\emph on
(x,y)
\emph default
.
\end_layout

\begin_layout Standard
This is a simple minimization problem for 
\begin_inset Formula $\theta_{0}\theta_{1}$
\end_inset

, where we minimize the cost function: 
\begin_inset Formula 
\[
J(\theta_{0},\theta_{1})=\frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^{2}
\]

\end_inset


\end_layout

\begin_layout Standard
For a multitude of lines that can be produced to regress the training data,
 there exists a cost function that describes the separation of the line
 from each of the points.
 We often simplify to take 
\begin_inset Formula $\theta_{0}=0$
\end_inset

, thus solving for only one variable.
 In case of two parameters, the plot becomes 2-dimensional.
 
\end_layout

\begin_layout Standard
Gradient descent is a generic machine learning algorithm for function minimizati
on.
 Starting with some set of parameters, keep on changing them until some
 minimum value of the cots function is reached.
 This guarantees to find some local optimum.
\end_layout

\begin_layout Standard
Repeat until convergence for 
\begin_inset Formula $j=0$
\end_inset

 and 
\begin_inset Formula $j=1$
\end_inset

 (in linear regression):
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\theta_{j}:=\theta_{j}-\alpha\frac{\partial}{\partial\theta_{j}}J(\theta_{0},\theta_{1})
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\alpha$
\end_inset

 is the learning rate.
 It expresses the size of the step we take on each iteration.
 Notice that both parametiers have to be updated simultaneously.
 Here the partial derivative expresses the slope of the plan at a given
 point.
 We want to subtract the slope times some constant 
\begin_inset Formula $\alpha$
\end_inset

 to approach the minimum.
 If the constant is small, the gradient descent will be slow.
 If it is too large, gradient descent can overshoot the minimum and may
 fail to converge.
\end_layout

\begin_layout Standard
For linear regression, the partial derivative works out as:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\theta_{0}=\frac{1}{m}\sum(h_{\theta}(x^{(i)})-y^{(i)})
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\theta_{1}=\frac{1}{m}\sum(h_{\theta}(x^{(i)})-y^{(i)})x^{(i)}
\]

\end_inset


\end_layout

\begin_layout Standard
Thus the parameters are estimated iteratively for each feature 
\begin_inset Formula $j$
\end_inset

 (until convergence):
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\theta_{j}:=\theta_{j}-\alpha\frac{1}{m}\sum(h_{\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)}
\]

\end_inset


\end_layout

\begin_layout Standard
For linear regression, the cost function is always a convex function, which
 means that it has only a single global minimum.

\emph on
 Batch
\emph default
 gradient descent is a particular case where we look at all the training
 examples when estimating parameters.
\end_layout

\begin_layout Section
Multivariate linear regression
\end_layout

\begin_layout Standard
The notation is slightly different: we have 
\begin_inset Formula $n$
\end_inset

 features, and 
\begin_inset Formula $x_{k}^{(i)}$
\end_inset

 denotes the 
\begin_inset Formula $k^{th}$
\end_inset

 feature of the 
\begin_inset Formula $i^{th}$
\end_inset

 sample datapoint.
 The form of the hypothesis is:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
h_{\theta}=\theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}...+\theta_{n}x_{n}
\]

\end_inset


\end_layout

\begin_layout Standard
We can think of this as 
\begin_inset Formula $x_{0}^{(i)}=1$
\end_inset

, so that we have a 
\begin_inset Formula $n+1$
\end_inset

 dimensional vector of variables and 
\begin_inset Formula $n+1$
\end_inset

 vector of parameters.
 This can be expressed as: 
\begin_inset Formula 
\[
\Theta^{T}X
\]

\end_inset


\end_layout

\begin_layout Standard
Gradient descent can be used to find the parameters.
 The cost function is:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
J(\theta_{0},...\theta_{n})=\frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^{2}
\]

\end_inset


\end_layout

\begin_layout Standard
Notice the square term in the formula.
\end_layout

\begin_layout Standard
In a problem with multiple features, it would make sense to scale the features
 in order to make the execution quicker.
 By scaling we mean using similar value.
 In terms of the contour plot, we need to circularize the plot.
\end_layout

\begin_layout Section
Linear algebra review
\end_layout

\begin_layout Subsection
Matrices and vectors
\end_layout

\begin_layout Standard
A matrix is a rectangular array of numbers
\end_layout

\begin_layout Standard
\begin_inset Formula $\begin{bmatrix}1 & 2 & 3 & 4\\
6 & 7 & 8 & 9\\
11 & 12 & 13 & 15\\
17 & 18 & 19 & 20\\
22 & 23 & 24 & 25
\end{bmatrix}$
\end_inset


\end_layout

\begin_layout Standard
The size of a matrix is written as 
\begin_inset Formula $rows\times columns$
\end_inset

.
 The matrix above 
\begin_inset Formula $A$
\end_inset

 is a 
\begin_inset Formula $5\times4$
\end_inset

 matrix, 
\begin_inset Formula $A\in\mathbb{R}^{5\times4}$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula $A_{ij}$
\end_inset

 means 
\begin_inset Formula $i^{th}$
\end_inset

 row and 
\begin_inset Formula $j^{th}$
\end_inset

 column.
 
\begin_inset Formula $A_{32}=12$
\end_inset

.
\end_layout

\begin_layout Standard
A vector is a special matrix that has only one column 
\begin_inset Formula $(n\times1)$
\end_inset

.
 N-dimensional vectors are just 
\begin_inset Formula $n\times1$
\end_inset

 matrices, which are elements of 
\begin_inset Formula $\mathbb{R}^{n}$
\end_inset

.
\end_layout

\begin_layout Subsection
Simple operations on matrices
\end_layout

\begin_layout Standard
Addition and multiplication are rather straightforward.
\end_layout

\begin_layout Standard
\begin_inset Formula $\begin{bmatrix}1 & 0\\
2 & 5\\
3 & 1
\end{bmatrix}$
\end_inset

+
\begin_inset Formula $\begin{bmatrix}4 & 0.5\\
2 & 5\\
0 & 1
\end{bmatrix}$
\end_inset

=
\begin_inset Formula $\begin{bmatrix}5 & 0.5\\
4 & 10\\
3 & 2
\end{bmatrix}$
\end_inset


\end_layout

\begin_layout Standard
The matrices need to be of the same dimension, and the result will be of
 the same dimension.
\end_layout

\begin_layout Standard
Multiplying by a scalar means multiplying every 
\begin_inset Formula $A_{ij}$
\end_inset

 by the scalar.
\end_layout

\begin_layout Standard
\begin_inset Formula $3\times\begin{bmatrix}1 & 0\\
2 & 5\\
3 & 1
\end{bmatrix}=\begin{bmatrix}3 & 0\\
6 & 15\\
9 & 3
\end{bmatrix}$
\end_inset


\end_layout

\begin_layout Standard
Notice that the dimensions should be the same.
 Also, 
\begin_inset Formula $k\times A=A\times k$
\end_inset


\end_layout

\begin_layout Standard
These operations can be combined in an arbitrarily complex manner 
\begin_inset Formula $\frac{kA+lB-nC}{kB+C}$
\end_inset

.
\end_layout

\begin_layout Subsection
Matrix vector multiplication
\end_layout

\begin_layout Standard
\begin_inset Formula $\begin{bmatrix}1 & 3\\
4 & 0\\
2 & 1
\end{bmatrix}\begin{bmatrix}1\\
5
\end{bmatrix}=\begin{bmatrix}16\\
4\\
7
\end{bmatrix}$
\end_inset


\end_layout

\begin_layout Standard
The multiplication of 
\begin_inset Formula $n\times k$
\end_inset

 matrix by 
\begin_inset Formula $k\times1$
\end_inset

 vector produces a 
\begin_inset Formula $n\times1$
\end_inset

 vector.
\end_layout

\begin_layout Standard
Matrix vector multiplication can be used to evaluate linear functions.
 Say we have a linear model 
\begin_inset Formula $h_{\theta}(x)=-40+0.25x$
\end_inset

.
 The dataset is: 
\begin_inset Formula $2104,1416,1534,852$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula $\begin{bmatrix}1 & 2104\\
1 & 1416\\
1 & 1534\\
1 & 852
\end{bmatrix}\begin{bmatrix}-40\\
0.25
\end{bmatrix}=\begin{bmatrix}486\\
314\\
343.5\\
173
\end{bmatrix}$
\end_inset


\end_layout

\begin_layout Subsection
Matrix matrix multiplication
\end_layout

\begin_layout Standard
Can be used for a solution to linear regression problem.
\end_layout

\begin_layout Standard
\begin_inset Formula $\begin{bmatrix}1 & 3 & 2\\
4 & 0 & 1
\end{bmatrix}\times\begin{bmatrix}1 & 3\\
0 & 1\\
5 & 2
\end{bmatrix}=\begin{bmatrix}11 & 10\\
9 & 14
\end{bmatrix}$
\end_inset


\end_layout

\begin_layout Standard
There is a particular arrangement of dimensions that is required: 
\begin_inset Formula $k\times l$
\end_inset

 by 
\begin_inset Formula $l\times m$
\end_inset

, giving a 
\begin_inset Formula $k\times m$
\end_inset

 matrix.
\end_layout

\begin_layout Standard
We can use multiplication to acquire multiple predictions, as shown above,
 where each column corresponds to a prediction based on some model.
\end_layout

\begin_layout Subsection
Properties of multiplication
\end_layout

\begin_layout Standard
Commutative property does not hold: 
\begin_inset Formula $A\times B\neq B\times A$
\end_inset


\end_layout

\begin_layout Standard
Associative property does hold: 
\begin_inset Formula $(A\times B)\times C=A\times(B\times C)$
\end_inset


\end_layout

\begin_layout Standard
The identity matrix 
\begin_inset Formula $I$
\end_inset

 is of the form for some dimension 
\begin_inset Formula $k$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula $\begin{bmatrix}1 & 0 & 0 & 0\\
0 & 1 & 0 & 0\\
0 & 0 & \ddots & \vdots\\
0 & 0 & \ldots & 1
\end{bmatrix}$
\end_inset


\end_layout

\begin_layout Standard
The identity matrix of some dimension has the following property:
\end_layout

\begin_layout Standard
\begin_inset Formula $A_{mn}\times I_{nn}=I_{mm}\times A_{mn}$
\end_inset


\end_layout

\begin_layout Subsection
Inverse and transpose
\end_layout

\begin_layout Standard
For real numbers, 1 serves as an identity value.
 Each number 
\begin_inset Formula $k$
\end_inset

 also has an inverse 
\begin_inset Formula $\frac{1}{k}$
\end_inset

, such that the number times its inverse equals identity.
 It is not defined for all numbers, such as the inverse of zero is not defined.
\end_layout

\begin_layout Standard
Matrices also have this property: 
\begin_inset Formula $AA^{-1}=A^{-1}A=I$
\end_inset

, where 
\begin_inset Formula $A$
\end_inset

 is a square matrix.
\end_layout

\begin_layout Standard
Transpose of a matrix is the matrix with rows becoming columns and columns
 becoming rows.
\end_layout

\end_body
\end_document
